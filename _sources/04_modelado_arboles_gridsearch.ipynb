{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c10a2bb4",
   "metadata": {},
   "source": [
    "# **4. Modelado**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caca1d8",
   "metadata": {},
   "source": [
    "## **1. Configuración inicial para modelado supervisado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b00ad123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_pos_weight (aprox): 2.768561872909699\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, f1_score, accuracy_score, confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Modelos gradiente\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Rutas\n",
    "ROOT = Path(\"..\")\n",
    "DATA = ROOT / \"data\" / \"processed\"\n",
    "MODELS = ROOT / \"models\"\n",
    "MODELS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Cargar datasets procesados de la Sección 3\n",
    "X_train = pd.read_parquet(DATA/\"X_train.parquet\")\n",
    "X_test  = pd.read_parquet(DATA/\"X_test.parquet\")\n",
    "y_train = pd.read_parquet(DATA/\"y_train.parquet\")[\"Churn\"].astype(\"int8\")\n",
    "y_test  = pd.read_parquet(DATA/\"y_test.parquet\")[\"Churn\"].astype(\"int8\")\n",
    "\n",
    "# Cargar preprocessor (ColumnTransformer)\n",
    "preprocessor = joblib.load(ROOT/\"models\"/\"preprocessor.joblib\")\n",
    "\n",
    "# Pos_weight para clases desbalanceadas (para XGB/LGBM)\n",
    "pos_weight = (len(y_train) - y_train.sum()) / y_train.sum()\n",
    "pos_weight = float(np.clip(pos_weight, 1.0, 20.0))  # acotar por seguridad\n",
    "print(\"scale_pos_weight (aprox):\", pos_weight)\n",
    "\n",
    "# CV y scoring\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = {\"roc_auc\":\"roc_auc\", \"f1\":\"f1\", \"accuracy\":\"accuracy\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e5abec",
   "metadata": {},
   "source": [
    "Ese valor (~2.7) indica que la clase Churn = 1 es unas 2.7 veces menos frecuente que la clase No Churn = 0, por lo que los modelos de gradiente deberán darle mayor peso a los errores en la clase positiva."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38105a72",
   "metadata": {},
   "source": [
    "## **2. Generación de *splits* crudos (data/interim) para Telco Churn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87f38cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: splits crudos guardados en data/interim/\n",
      "Distribución y_train: {0: 0.735, 1: 0.265}\n",
      "Distribución y_test : {0: 0.735, 1: 0.265}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ROOT = Path(\"..\")\n",
    "DATA = ROOT / \"data\"\n",
    "PROC = DATA / \"processed\"\n",
    "INTERIM = DATA / \"interim\"\n",
    "INTERIM.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TARGET = \"Churn\"\n",
    "ID_COL = \"customerID\"\n",
    "\n",
    "# 1) Carga del parquet limpio del EDA\n",
    "df_raw = pd.read_parquet(PROC / \"telco_churn.parquet\")\n",
    "\n",
    "# 2) Normalizaciones mínimas\n",
    "# 2.1 Target a 0/1 (robusto a mayúsculas/acentos/espacios)\n",
    "t = df_raw[TARGET].astype(str).str.strip().str.lower()\n",
    "mapa = {\n",
    "    \"yes\": 1, \"no\": 0,\n",
    "    \"sí\": 1, \"si\": 1,\n",
    "    \"true\": 1, \"false\": 0, \"0\": 0, \"1\": 1\n",
    "}\n",
    "df_raw[TARGET] = t.map(mapa)\n",
    "\n",
    "# Si quedó algo sin mapear, avisa para inspección\n",
    "resto = df_raw.loc[df_raw[TARGET].isna(), TARGET]\n",
    "if len(resto):\n",
    "    print(\"Valores no mapeados en Churn ->\", df_raw.loc[df_raw[TARGET].isna(), TARGET].unique())\n",
    "    # Si sabes que los NaN equivalen a 'No', puedes descomentar:\n",
    "    # df_raw[TARGET] = df_raw[TARGET].fillna(0)\n",
    "\n",
    "df_raw[TARGET] = df_raw[TARGET].astype(\"int8\")\n",
    "\n",
    "# 2.2 TotalCharges a numérico (en este dataset suele haber strings vacíos)\n",
    "if \"TotalCharges\" in df_raw.columns:\n",
    "    df_raw[\"TotalCharges\"] = pd.to_numeric(df_raw[\"TotalCharges\"], errors=\"coerce\")\n",
    "\n",
    "# 3) X/y crudos (sin procesar)\n",
    "X_raw = df_raw.drop(columns=[TARGET])\n",
    "y_raw = df_raw[TARGET]\n",
    "\n",
    "# 4) Split estratificado\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(\n",
    "    X_raw, y_raw,\n",
    "    test_size=0.20, random_state=42, stratify=y_raw\n",
    ")\n",
    "\n",
    "# 5) Guardar para la sección de modelado (pipeline con preprocessor)\n",
    "X_train_raw.to_parquet(INTERIM/\"X_train_raw.parquet\", index=False)\n",
    "X_test_raw.to_parquet(INTERIM/\"X_test_raw.parquet\", index=False)\n",
    "y_train_raw.to_frame(TARGET).to_parquet(INTERIM/\"y_train_raw.parquet\", index=False)\n",
    "y_test_raw.to_frame(TARGET).to_parquet(INTERIM/\"y_test_raw.parquet\", index=False)\n",
    "\n",
    "print(\"OK: splits crudos guardados en data/interim/\")\n",
    "print(\"Distribución y_train:\", y_train_raw.value_counts(normalize=True).round(3).to_dict())\n",
    "print(\"Distribución y_test :\", y_test_raw.value_counts(normalize=True).round(3).to_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad4400c",
   "metadata": {},
   "source": [
    "Esas proporciones indican que aproximadamente el 26.5 % de los clientes hacen churn y el 73.5 % se mantiene, tanto en entrenamiento como en prueba, gracias al stratify=y_raw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31c9474f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (5634, 20) (1409, 20)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "ROOT = Path(\"..\")\n",
    "INTERIM = ROOT / \"data\" / \"interim\"\n",
    "MODELS = ROOT / \"models\"\n",
    "\n",
    "# Carga splits crudos\n",
    "X_train = pd.read_parquet(INTERIM/\"X_train_raw.parquet\")\n",
    "X_test  = pd.read_parquet(INTERIM/\"X_test_raw.parquet\")\n",
    "y_train = pd.read_parquet(INTERIM/\"y_train_raw.parquet\")[\"Churn\"].astype(\"int8\")\n",
    "y_test  = pd.read_parquet(INTERIM/\"y_test_raw.parquet\")[\"Churn\"].astype(\"int8\")\n",
    "\n",
    "# Carga tu preprocessor.joblib (hecho en sección 3)\n",
    "preprocessor = joblib.load(MODELS/\"preprocessor.joblib\")\n",
    "\n",
    "print(\"Shapes:\", X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8503043f",
   "metadata": {},
   "source": [
    "## **3. Resultados de la búsqueda de hiperparámetros para Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7644482c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "RF best ROC-AUC: 0.8442564977030317\n",
      "RF best params: {'clf__bootstrap': True, 'clf__criterion': 'gini', 'clf__max_features': 'log2', 'clf__min_samples_leaf': 4, 'clf__min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "rf_pipe = Pipeline(steps=[\n",
    "    (\"pre\", preprocessor),\n",
    "    (\"clf\", RandomForestClassifier(\n",
    "        n_estimators=400,\n",
    "        class_weight=\"balanced\",   # ayuda con desbalance\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "rf_grid = {\n",
    "    \"clf__max_features\": [\"sqrt\", \"log2\"],\n",
    "    \"clf__min_samples_split\": [2, 5, 10],\n",
    "    \"clf__min_samples_leaf\": [1, 2, 4],\n",
    "    \"clf__bootstrap\": [True],\n",
    "    \"clf__criterion\": [\"gini\", \"entropy\"]  # también 'log_loss' en nuevas versiones\n",
    "}\n",
    "\n",
    "rf_search = GridSearchCV(\n",
    "    rf_pipe, rf_grid, cv=cv, scoring=scoring, refit=\"roc_auc\",\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "rf_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"RF best ROC-AUC:\", rf_search.best_score_)\n",
    "print(\"RF best params:\", rf_search.best_params_)\n",
    "\n",
    "# Guardar\n",
    "joblib.dump(rf_search.best_estimator_, MODELS/\"rf_best.joblib\")\n",
    "pd.DataFrame(rf_search.cv_results_).to_csv(MODELS/\"rf_cv_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85fd88e",
   "metadata": {},
   "source": [
    "La salida muestra el resumen del `GridSearchCV` aplicado al pipeline con **RandomForestClassifier**:\n",
    "\n",
    "- `Fitting 5 folds for each of 36 candidates, totalling 180 fits`  \n",
    "  - Se definió una **búsqueda en malla** (`GridSearchCV`) con:\n",
    "    - 36 combinaciones distintas de hiperparámetros (`36 candidates`).\n",
    "    - Validación cruzada estratificada de **5 folds**.  \n",
    "  - En total se entrenaron **180 modelos** (36 combinaciones × 5 folds).\n",
    "\n",
    "- `RF best ROC-AUC: 0.8442564977030317`  \n",
    "  - El mejor modelo encontrado alcanzó un **ROC-AUC ≈ 0.844**, lo que indica una **buena capacidad de discriminación** entre clientes que hacen churn (1) y los que no (0).  \n",
    "  - Mientras más se acerque a 1.0, mejor separa las clases; valores > 0.8 suelen considerarse bastante buenos en muchos problemas reales.\n",
    "\n",
    "- `RF best params: {...}`  \n",
    "  El mejor modelo usa los siguientes **hiperparámetros**:\n",
    "\n",
    "  - `clf__bootstrap: True`  \n",
    "    El Random Forest se entrena con *bootstrap* (muestreo con reemplazo) para cada árbol, lo típico en RF estándar.\n",
    "\n",
    "  - `clf__criterion: 'gini'`  \n",
    "    La impureza de los nodos se mide con el índice de Gini (en vez de `entropy`).  \n",
    "    Esto afecta cómo se hacen las particiones en los árboles.\n",
    "\n",
    "  - `clf__max_features: 'log2'`  \n",
    "    En cada split, el árbol considera solo `log2(n_features)` variables candidatas.  \n",
    "    Esto introduce más aleatoriedad y puede mejorar generalización.\n",
    "\n",
    "  - `clf__min_samples_leaf: 4`  \n",
    "    Cada hoja del árbol debe tener al menos **4 observaciones**.  \n",
    "    Hojas con mínimo mayor reducen el sobreajuste (evitan hojas muy pequeñas).\n",
    "\n",
    "  - `clf__min_samples_split: 2`  \n",
    "    Un nodo se puede dividir si tiene al menos **2 muestras**.  \n",
    "    Combinado con `min_samples_leaf=4`, permite cierta profundidad pero sigue controlando el tamaño mínimo de hojas.\n",
    "\n",
    "En resumen, de todas las combinaciones probadas, el modelo con estos hiperparámetros fue el que logró **mayor ROC-AUC media en CV (≈ 0.844)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a02e18",
   "metadata": {},
   "source": [
    "## **4. Resultados de la búsqueda de hiperparámetros para XGBoost (XGBClassifier)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57defa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1296 candidates, totalling 6480 fits\n",
      "XGB best ROC-AUC: 0.848440533786993\n",
      "XGB best params: {'clf__colsample_bytree': 0.7, 'clf__gamma': 1.0, 'clf__max_depth': 3, 'clf__min_child_weight': 3, 'clf__reg_alpha': 1.0, 'clf__reg_lambda': 2.0, 'clf__scale_pos_weight': 1.0, 'clf__subsample': 0.9}\n"
     ]
    }
   ],
   "source": [
    "xgb_pipe = Pipeline(steps=[\n",
    "    (\"pre\", preprocessor),\n",
    "    (\"clf\", XGBClassifier(\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.05,\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"auc\",\n",
    "        tree_method=\"hist\",      # 'gpu_hist' si tienes GPU\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "xgb_grid = {\n",
    "    \"clf__max_depth\": [3, 5, 7],\n",
    "    \"clf__min_child_weight\": [1, 3, 5],\n",
    "    \"clf__gamma\": [0, 0.5, 1.0],\n",
    "    \"clf__reg_alpha\": [0, 0.5, 1.0],\n",
    "    \"clf__reg_lambda\": [1.0, 2.0],\n",
    "    \"clf__scale_pos_weight\": [1.0, pos_weight],  # maneja desbalance\n",
    "    \"clf__subsample\": [0.7, 0.9],\n",
    "    \"clf__colsample_bytree\": [0.7, 0.9]\n",
    "}\n",
    "\n",
    "xgb_search = GridSearchCV(\n",
    "    xgb_pipe, xgb_grid, cv=cv, scoring=scoring, refit=\"roc_auc\",\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "xgb_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"XGB best ROC-AUC:\", xgb_search.best_score_)\n",
    "print(\"XGB best params:\", xgb_search.best_params_)\n",
    "\n",
    "joblib.dump(xgb_search.best_estimator_, MODELS/\"xgb_best.joblib\")\n",
    "pd.DataFrame(xgb_search.cv_results_).to_csv(MODELS/\"xgb_cv_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529c7150",
   "metadata": {},
   "source": [
    "La salida corresponde al `GridSearchCV` aplicado al pipeline con **XGBClassifier**:\n",
    "\n",
    "- `Fitting 5 folds for each of 1296 candidates, totalling 6480 fits`  \n",
    "  - El grid de hiperparámetros tiene **1.296 combinaciones distintas** (`1296 candidates`).  \n",
    "  - Se utilizó validación cruzada estratificada de **5 folds**, por lo que se entrenaron:  \n",
    "    \\[\n",
    "    1296 combinaciones  folds = 6480 \n",
    "    \\]\n",
    "  - Esto implica una búsqueda mucho más exhaustiva que la de Random Forest, por eso el número de *fits* es tan alto.\n",
    "\n",
    "- `XGB best ROC-AUC: 0.848440533786993`  \n",
    "  - El mejor modelo de XGBoost alcanzó un **ROC-AUC ≈ 0.848**, ligeramente mejor que el obtenido con Random Forest (~0.844).  \n",
    "  - Un ROC-AUC en este rango indica que el modelo tiene **muy buena capacidad para separar clientes que hacen churn (1) de los que no (0)**, midiendo el trade-off entre TPR y FPR a través de diferentes umbrales.\n",
    "\n",
    "- `XGB best params: {...}`  \n",
    "  Los hiperparámetros óptimos encontrados son:\n",
    "\n",
    "  - `clf__colsample_bytree: 0.7`  \n",
    "    Cada árbol solo ve el **70 % de las variables** en cada split, lo que introduce aleatoriedad y ayuda a evitar sobreajuste.\n",
    "\n",
    "  - `clf__gamma: 1.0`  \n",
    "    Controla el **mínimo descenso en la pérdida** requerido para hacer una nueva partición.  \n",
    "    Un valor mayor (1.0) hace que el árbol sea más conservador, evitando splits que no mejoren suficientemente el modelo.\n",
    "\n",
    "  - `clf__max_depth: 3`  \n",
    "    Profundidad máxima de los árboles = 3.  \n",
    "    Árboles poco profundos suelen **generalizar mejor** y reducen el riesgo de sobreajuste.\n",
    "\n",
    "  - `clf__min_child_weight: 3`  \n",
    "    Peso mínimo de las instancias en un nodo hijo.  \n",
    "    Un valor 3 también hace que el modelo sea más conservador, evitando hojas con muy pocas observaciones.\n",
    "\n",
    "  - `clf__reg_alpha: 1.0`  \n",
    "    **Regularización L1** distinta de cero, que induce cierta sparsidad en los pesos y ayuda a controlar el sobreajuste.\n",
    "\n",
    "  - `clf__reg_lambda: 2.0`  \n",
    "    **Regularización L2** relativamente fuerte, penalizando coeficientes grandes y contribuyendo a la estabilidad del modelo.\n",
    "\n",
    "  - `clf__scale_pos_weight: 1.0`  \n",
    "    Aunque probaste incluir el `pos_weight` teórico para el desbalance, la búsqueda encontró que el mejor desempeño es con **peso 1.0**, es decir, sin reponderar adicionalmente la clase positiva en XGBoost (probablemente porque el desbalance no es extremo o ya está manejado indirectamente).\n",
    "\n",
    "  - `clf__subsample: 0.9`  \n",
    "    Cada árbol se entrena con el **90 % de las filas** (submuestreo de instancias), lo que introduce variabilidad y ayuda a mejorar la generalización.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac34b291",
   "metadata": {},
   "source": [
    "## **5. Resultados de la búsqueda de hiperparámetros con LightGBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2dc4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(ColumnTransformer(transformers=[('num', SimpleImputer(strategy='median'),\n",
      "                                 ['SeniorCitizen', 'tenure', 'MonthlyCharges',\n",
      "                                  'TotalCharges']),\n",
      "                                ('cat',\n",
      "                                 OneHotEncoder(handle_unknown='ignore',\n",
      "                                               sparse_output=False),\n",
      "                                 ['gender', 'Partner', 'Dependents',\n",
      "                                  'PhoneService', 'MultipleLines',\n",
      "                                  'InternetService', 'OnlineSecurity',\n",
      "                                  'OnlineBackup', 'DeviceProtection',\n",
      "                                  'TechSupport', 'StreamingTV',\n",
      "                                  'StreamingMovies', 'Contract',\n",
      "                                  'PaperlessBilling', 'PaymentMethod'])],\n",
      "                  verbose_feature_names_out=False), \n",
      "      customerID  gender  SeniorCitizen Partner Dependents  tenure  \\\n",
      "0     4950-BDEUX    Male              0      No         No      35   \n",
      "1     7993-NQLJE    Male              0     Yes        Yes      15   \n",
      "2     7321-ZNSLA    Male              0     Yes        Yes      13   \n",
      "3     4922-CVPDX  Female              0     Yes         No      26   \n",
      "4     2903-YYTBW    Male              0     Yes        Yes       1   \n",
      "...          ...     ...            ...     ...        ...     ...   \n",
      "5629  6308-CQRBU  Female              0     Yes         No      71   \n",
      "5630  2842-JTCCU    Male              0      No         No       2   \n",
      "5631  6402-ZFPPI  Female              1      No         No      25   \n",
      "..., \n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "5629    0\n",
      "5630    1\n",
      "5631    1\n",
      "5632    0\n",
      "5633    0\n",
      "Name: Churn, Length: 5634, dtype: int8, weight=None, message_clsname='Pipeline', message=None, params={ 'decision_function': {},\n",
      "  'fit': {},\n",
      "  'fit_predict': {},\n",
      "  'fit_transform': {},\n",
      "  'inverse_transform': {},\n",
      "  'partial_fit': {},\n",
      "  'predict': {},\n",
      "  'predict_log_proba': {},\n",
      "  'predict_proba': {},\n",
      "  'score': {},\n",
      "  'split': {},\n",
      "  'transform': {}})\n",
      "________________________________________________fit_transform_one - 0.0s, 0.0min\n",
      "[LightGBM] [Info] Number of positive: 1495, number of negative: 4139\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000528 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 667\n",
      "[LightGBM] [Info] Number of data points in the train set: 5634, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Best ROC-AUC (cv=5): 0.8339218758605339\n",
      "Best params (cv=5): {'clf__class_weight': 'balanced', 'clf__colsample_bytree': 0.8, 'clf__max_bin': 255, 'clf__min_child_samples': 20, 'clf__num_leaves': 31, 'clf__reg_alpha': 0.5, 'clf__reg_lambda': 0.5, 'clf__subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from lightgbm import LGBMClassifier\n",
    "from joblib import Memory\n",
    "from pathlib import Path\n",
    "\n",
    "# 5 particiones estratificadas (requisito)\n",
    "cv5 = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = {\"roc_auc\":\"roc_auc\",\"f1\":\"f1\",\"accuracy\":\"accuracy\"}\n",
    "\n",
    "# cachea el preprocesamiento para no rehacer OHE/imputación en cada fold\n",
    "cache_dir = Path(\"../.cache_sklearn\"); cache_dir.mkdir(exist_ok=True)\n",
    "pipe_lgb = Pipeline(\n",
    "    steps=[(\"pre\", preprocessor),\n",
    "           (\"clf\", LGBMClassifier(\n",
    "               n_estimators=500,\n",
    "               learning_rate=0.05,\n",
    "               objective=\"binary\",\n",
    "               random_state=42,\n",
    "               n_jobs=-1\n",
    "           ))],\n",
    "    memory=Memory(cache_dir)\n",
    ")\n",
    "\n",
    "# Grid FINAL pequeño (12–24 combinaciones aprox). Ajusta si quieres 16–20 como tope.\n",
    "param_grid_final = {\n",
    "    \"clf__num_leaves\": [31, 63],\n",
    "    \"clf__min_child_samples\": [20, 50],\n",
    "    \"clf__reg_alpha\": [0.0, 0.5],\n",
    "    \"clf__reg_lambda\": [0.0, 0.5],\n",
    "    \"clf__subsample\": [0.8],\n",
    "    \"clf__colsample_bytree\": [0.8],\n",
    "    \"clf__class_weight\": [None, \"balanced\"],  # ayuda con desbalance\n",
    "    \"clf__max_bin\": [255],                    # fijo para acelerar\n",
    "}\n",
    "\n",
    "search5 = GridSearchCV(\n",
    "    estimator=pipe_lgb,\n",
    "    param_grid=param_grid_final,\n",
    "    cv=cv5,\n",
    "    scoring=scoring,\n",
    "    refit=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "search5.fit(X_train, y_train)\n",
    "print(\"Best ROC-AUC (cv=5):\", search5.best_score_)\n",
    "print(\"Best params (cv=5):\", search5.best_params_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02593db1",
   "metadata": {},
   "source": [
    " Resumen de la búsqueda\n",
    "\n",
    "- `Fitting 5 folds for each of 32 candidates, totalling 160 fits`  \n",
    "  - El grid de hiperparámetros tiene **32 combinaciones diferentes**.  \n",
    "  - Con validación cruzada estratificada de **5 folds**, se entrenan:  \n",
    "    \\[\n",
    "    32 \\text{ modelos} \\times 5 \\text{ folds} = 160 \\text{ entrenamientos}\n",
    "    \\]\n",
    "  - Esto cumple el requisito de **5 particiones estratificadas**.\n",
    "\n",
    "\n",
    "\n",
    " Mensajes de `Memory` y del Pipeline\n",
    "\n",
    "- El bloque largo que comienza con:\n",
    "  - `[Memory] Calling sklearn.pipeline._fit_transform_one...`  \n",
    "    indica que el `Pipeline` está usando **caché** (`memory=Memory(cache_dir)`):  \n",
    "    - El preprocesador (`ColumnTransformer` con imputación + OneHotEncoder) se ejecuta una vez y se guarda en disco.  \n",
    "    - En los siguientes folds de la CV no tiene que recalcular todo el OHE/imputación desde cero, lo que **acelera bastante** la búsqueda.\n",
    "\n",
    "- Dentro de ese mensaje se ven:\n",
    "  - El `ColumnTransformer` con:\n",
    "    - Numéricas: `SeniorCitizen`, `tenure`, `MonthlyCharges`, `TotalCharges` (imputadas con mediana).  \n",
    "    - Categóricas: todas las variables de tipo categoría codificadas con `OneHotEncoder(handle_unknown='ignore')`.  \n",
    "  - También se listan algunas filas de ejemplo del dataset de entrada y el vector `y` (la columna `Churn`).\n",
    "\n",
    "\n",
    "\n",
    " Logs de LightGBM\n",
    "\n",
    "Los mensajes tipo:\n",
    "\n",
    "- `[LightGBM] [Info] Number of positive: 1495, number of negative: 4139`  \n",
    "  - Muestran el **nº de observaciones de la clase positiva y negativa** en el conjunto de entrenamiento (desbalance moderado).\n",
    "\n",
    "- `[LightGBM] [Info] Total Bins 667`  \n",
    "  - Cantidad total de *bins* generados internamente para discretizar las features.\n",
    "\n",
    "- `[LightGBM] [Info] Number of data points in the train set: 5634, number of used features: 45`  \n",
    "  - Se están usando **5634 filas** y **45 variables** luego de todo el preprocesamiento (OHE incluido).\n",
    "\n",
    "- `[LightGBM] [Info] Start training from score 0.000000`  \n",
    "  - LightGBM inicializa el modelo asumiendo un score base (probabilidad media inicial de la clase positiva).\n",
    "\n",
    "Estos mensajes son puramente informativos y confirman que el modelo está entrenando correctamente.\n",
    "\n",
    "\n",
    "\n",
    " Métrica principal obtenida\n",
    "\n",
    "- `Best ROC-AUC (cv=5): 0.8339218758605339`  \n",
    "  - El mejor modelo encontrado con el grid actual alcanza un **ROC-AUC ≈ 0.834** promedio en validación cruzada de 5 folds.  \n",
    "  - Esto indica una **buena capacidad discriminativa**, aunque algo por debajo del XGBoost (~0.848) y del Random Forest (~0.844) que entrenaste antes.  \n",
    "  - Aun así, es un modelo competitivo y sirve como **tercer benchmark fuerte**.\n",
    "\n",
    "\n",
    "\n",
    " Mejores hiperparámetros encontrados\n",
    "\n",
    "- `Best params (cv=5): {'clf__class_weight': 'balanced', 'clf__colsample_bytree': 0.8, 'clf__max_bin': 255, 'clf__min_child_samples': 20, 'clf__num_leaves': 31, 'clf__reg_alpha': 0.5, 'clf__reg_lambda': 0.5, 'clf__subsample': 0.8}`  \n",
    "\n",
    "Interpretación de cada uno:\n",
    "\n",
    "- `clf__class_weight: 'balanced'`  \n",
    "  - LightGBM **pondera más la clase minoritaria (Churn=1)** para compensar el desbalance (1495 vs 4139).  \n",
    "  - Esto suele mejorar métricas como F1 y recall de la clase positiva.\n",
    "\n",
    "- `clf__num_leaves: 31`  \n",
    "  - Número máximo de hojas en cada árbol.  \n",
    "  - 31 es un valor moderado: árboles relativamente simples → **mejor generalización y menos sobreajuste**.\n",
    "\n",
    "- `clf__min_child_samples: 20`  \n",
    "  - Mínimo de ejemplos que debe tener una hoja.  \n",
    "  - Obliga a que las hojas tengan al menos 20 muestras, haciendo el modelo más conservador.\n",
    "\n",
    "- `clf__reg_alpha: 0.5` y `clf__reg_lambda: 0.5`  \n",
    "  - **Regularización L1 (`alpha`) y L2 (`lambda`)** distintas de cero.  \n",
    "  - Ambas penalizan la complejidad del modelo y ayudan a controlar el sobreajuste.\n",
    "\n",
    "- `clf__subsample: 0.8`  \n",
    "  - Cada árbol se entrena con un **80 % de las filas**, introduciendo aleatoriedad tipo *bagging*, lo que mejora la robustez del modelo.\n",
    "\n",
    "- `clf__colsample_bytree: 0.8`  \n",
    "  - Cada árbol ve sólo el **80 % de las variables** ⇒ reduce correlación entre árboles y también ayuda a la generalización.\n",
    "\n",
    "- `clf__max_bin: 255`  \n",
    "  - Número máximo de bins para discretizar las features.  \n",
    "  - 255 es el valor típico por defecto y se fijó para **acelerar** el entrenamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d5b377",
   "metadata": {},
   "source": [
    "## **6. Verificación de rutas del proyecto y modelos disponibles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de505cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: c:\\Users\\juana\\MLOPS\\miniproyecto6\\docs\n",
      "PROJECT: c:\\Users\\juana\\MLOPS\\miniproyecto6\n",
      "MODELS: c:\\Users\\juana\\MLOPS\\miniproyecto6\\models\n",
      "Modelos disponibles: [WindowsPath('c:/Users/juana/MLOPS/miniproyecto6/models/preprocessor.joblib'), WindowsPath('c:/Users/juana/MLOPS/miniproyecto6/models/rf_best.joblib'), WindowsPath('c:/Users/juana/MLOPS/miniproyecto6/models/xgb_best.joblib')]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CWD = Path.cwd()\n",
    "# Si corres dentro de docs/, sube a la raíz del proyecto:\n",
    "PROJECT = CWD if CWD.name != \"docs\" else CWD.parent\n",
    "MODELS = PROJECT / \"models\"\n",
    "MODELS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"CWD:\", CWD)\n",
    "print(\"PROJECT:\", PROJECT)\n",
    "print(\"MODELS:\", MODELS)\n",
    "print(\"Modelos disponibles:\", list(MODELS.glob(\"*.joblib\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bced587b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ROC-AUC (cv=5): 0.8339218758605339\n",
      "Best params (cv=5): {'clf__class_weight': 'balanced', 'clf__colsample_bytree': 0.8, 'clf__max_bin': 255, 'clf__min_child_samples': 20, 'clf__num_leaves': 31, 'clf__reg_alpha': 0.5, 'clf__reg_lambda': 0.5, 'clf__subsample': 0.8}\n",
      "Guardados:\n",
      " - c:\\Users\\juana\\MLOPS\\miniproyecto6\\models\\lgbm_best.joblib\n",
      " - c:\\Users\\juana\\MLOPS\\miniproyecto6\\models\\lgbm_cv_results.csv\n",
      " - c:\\Users\\juana\\MLOPS\\miniproyecto6\\models\\lgbm_clf_only.joblib\n",
      "Ahora hay: [WindowsPath('c:/Users/juana/MLOPS/miniproyecto6/models/lgbm_best.joblib'), WindowsPath('c:/Users/juana/MLOPS/miniproyecto6/models/lgbm_clf_only.joblib'), WindowsPath('c:/Users/juana/MLOPS/miniproyecto6/models/preprocessor.joblib'), WindowsPath('c:/Users/juana/MLOPS/miniproyecto6/models/rf_best.joblib'), WindowsPath('c:/Users/juana/MLOPS/miniproyecto6/models/xgb_best.joblib')]\n"
     ]
    }
   ],
   "source": [
    "import joblib, pandas as pd\n",
    "\n",
    "print(\"Best ROC-AUC (cv=5):\", search5.best_score_)\n",
    "print(\"Best params (cv=5):\", search5.best_params_)\n",
    "\n",
    "# 2.1) Guardar el MEJOR PIPELINE (preprocesador + lgbm)\n",
    "lgbm_best_path = MODELS / \"lgbm_best.joblib\"\n",
    "joblib.dump(search5.best_estimator_, lgbm_best_path)\n",
    "\n",
    "# 2.2) Guardar resultados de la CV\n",
    "cv_path = MODELS / \"lgbm_cv_results.csv\"\n",
    "pd.DataFrame(search5.cv_results_).to_csv(cv_path, index=False)\n",
    "\n",
    "# 2.3) (Opcional) Guardar SOLO el clasificador ya ajustado\n",
    "clf_only_path = MODELS / \"lgbm_clf_only.joblib\"\n",
    "best_clf = search5.best_estimator_.named_steps[\"clf\"]\n",
    "joblib.dump(best_clf, clf_only_path)\n",
    "\n",
    "print(\"Guardados:\")\n",
    "print(\" -\", lgbm_best_path)\n",
    "print(\" -\", cv_path)\n",
    "print(\" -\", clf_only_path)\n",
    "print(\"Ahora hay:\", list(MODELS.glob('*.joblib')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd73733",
   "metadata": {},
   "source": [
    "## **7. Comparación final de modelos en el conjunto de prueba (RF vs XGB vs LGBM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40627b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>0.839673</td>\n",
       "      <td>0.624277</td>\n",
       "      <td>0.769340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>0.846081</td>\n",
       "      <td>0.579666</td>\n",
       "      <td>0.803407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbm</th>\n",
       "      <td>0.829939</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.761533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       roc_auc        f1  accuracy\n",
       "rf    0.839673  0.624277  0.769340\n",
       "xgb   0.846081  0.579666  0.803407\n",
       "lgbm  0.829939  0.611111  0.761533"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates = {\n",
    "    \"rf\": joblib.load(MODELS/\"rf_best.joblib\"),\n",
    "    \"xgb\": joblib.load(MODELS/\"xgb_best.joblib\"),\n",
    "    \"lgbm\": joblib.load(MODELS/\"lgbm_best.joblib\"),\n",
    "}\n",
    "\n",
    "scores = {}\n",
    "for name, model in candidates.items():\n",
    "    prob = model.predict_proba(X_test)[:,1]\n",
    "    pred = (prob >= 0.5).astype(int)\n",
    "    scores[name] = {\n",
    "        \"roc_auc\": roc_auc_score(y_test, prob),\n",
    "        \"f1\": f1_score(y_test, pred),\n",
    "        \"accuracy\": accuracy_score(y_test, pred)\n",
    "    }\n",
    "pd.DataFrame(scores).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690375a6",
   "metadata": {},
   "source": [
    "Interpretación\n",
    "\n",
    "- **XGBoost (xgb)** es tu mejor modelo global:  \n",
    "  - Tiene el **ROC-AUC más alto (0.846)** → mejor capacidad de separar clientes que harán *churn*.  \n",
    "  - Tiene también la **mejor accuracy (0.803)**.  \n",
    "  - Su F1 es inferior a RF pero aceptable, considerando el desbalance.\n",
    "\n",
    "- **Random Forest (rf)** muestra:\n",
    "  - Buen F1 (el mejor de los tres)  \n",
    "  - Pero menor ROC-AUC y accuracy en comparación con XGB.\n",
    "\n",
    "- **LightGBM (lgbm)**:\n",
    "  - Es el que peor se desempeña en este caso.  \n",
    "  - Aunque no está mal, queda por detrás de XGB y RF en todas las métricas.\n",
    "\n",
    "\n",
    "\n",
    "###  Conclusión final\n",
    "\n",
    "**El mejor modelo para tu caso de Churn es XGBoost**, ya que:\n",
    "- Generaliza mejor (mejor ROC-AUC y accuracy en test)\n",
    "- Maneja bien el desbalance\n",
    "- Logra la mejor separación entre clases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccac4ef",
   "metadata": {},
   "source": [
    "## **8. Evaluación final del mejor modelo (XGBoost) en el conjunto de prueba**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7837f345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor en test: xgb {'roc_auc': np.float64(0.8460810147510915), 'f1': 0.5796661608497724, 'accuracy': 0.8034066713981547}\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.91      0.87      1035\n",
      "           1       0.67      0.51      0.58       374\n",
      "\n",
      "    accuracy                           0.80      1409\n",
      "   macro avg       0.75      0.71      0.73      1409\n",
      "weighted avg       0.79      0.80      0.79      1409\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred No</th>\n",
       "      <th>Pred Sí</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No Churn</th>\n",
       "      <td>941</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Churn</th>\n",
       "      <td>183</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pred No  Pred Sí\n",
       "No Churn      941       94\n",
       "Churn         183      191"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Escoge el mejor por ROC-AUC y muestra reporte\n",
    "best_name = max(scores, key=lambda k: scores[k][\"roc_auc\"])\n",
    "best_model = candidates[best_name]\n",
    "print(\"Mejor en test:\", best_name, scores[best_name])\n",
    "\n",
    "y_prob = best_model.predict_proba(X_test)[:,1]\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_df = pd.DataFrame(cm, index=[\"No Churn\",\"Churn\"], columns=[\"Pred No\",\"Pred Sí\"])\n",
    "cm_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb57a3e",
   "metadata": {},
   "source": [
    "\n",
    " Clase 0 (No Churn)\n",
    "- **Precision 0.84:** de los usuarios predichos como *No Churn*, el 84% realmente no se va.  \n",
    "- **Recall 0.91:** captura correctamente el 91% de los clientes que NO hacen churn.  \n",
    "- **F1 0.87:** excelente equilibrio.\n",
    "\n",
    "Esto significa que el modelo es **muy bueno identificando clientes que se quedan**.\n",
    "\n",
    "\n",
    "- **Precision 0.67:** cuando el modelo dice que alguien hará churn, acierta un 67%.  \n",
    "- **Recall 0.51:** solo identifica correctamente al 51% de los clientes que sí se irán.  \n",
    "- **F1 0.58:** rendimiento moderado.\n",
    "\n",
    "Esto refleja la dificultad del problema:  \n",
    "**es más difícil detectar a los clientes que sí se van**, típico en datasets desbalanceados.\n",
    " Aciertos:\n",
    "- **941** No Churn clasificados correctamente.\n",
    "- **191** Churn clasificados correctamente.\n",
    "\n",
    " Errores:\n",
    "- **94** clientes que se quedan, el modelo los marcó como churn (falsos positivos).  \n",
    "- **183** clientes que sí se van, el modelo NO los detectó (falsos negativos).\n",
    "\n",
    " **Los falsos negativos (183)** son especialmente importantes para empresas,  \n",
    "porque representan clientes que sí abandonan pero el modelo no los alerta.\n",
    "\n",
    "Conclusión general\n",
    "\n",
    "El modelo XGBoost:\n",
    "\n",
    "- Es **excelente identificando a los clientes que no se van** (Recall = 0.91).\n",
    "- Tiene **buen poder de separación global** (ROC-AUC = 0.846).\n",
    "- **Detecta alrededor de la mitad de los churn reales**, lo cual es razonable pero puede mejorarse ajustando el threshold o aplicando técnicas adicionales (SMOTE, focal loss, calibration, cost-sensitive learning).\n",
    "\n",
    "Este es un **resultado sólido y típico** para un problema real de churn con desbalance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23782af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Guardados:\n",
      " - c:\\Users\\juana\\MLOPS\\miniproyecto6\\docs\\models\\preprocessor.joblib\n",
      " - c:\\Users\\juana\\MLOPS\\miniproyecto6\\docs\\models\\xgb_clf_only.joblib\n",
      " - c:\\Users\\juana\\MLOPS\\miniproyecto6\\docs\\models\\feature_names.json\n"
     ]
    }
   ],
   "source": [
    "# === Paths (ajústalos si ya los tienes definidos) ===\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "MODELS = ROOT / \"models\"\n",
    "MODELS.mkdir(exist_ok=True)\n",
    "\n",
    "# 1) Extraer el MEJOR PIPELINE del grid\n",
    "best_pipe = xgb_search.best_estimator_     # <--- Pipeline(pre=..., clf=...)\n",
    "\n",
    "# 2) Separar artefactos\n",
    "preprocessor = best_pipe.named_steps[\"pre\"]\n",
    "clf_only     = best_pipe.named_steps[\"clf\"]\n",
    "\n",
    "# 3) Guardar por separado (sobrescribe si ya existen)\n",
    "joblib.dump(preprocessor, MODELS / \"preprocessor.joblib\")\n",
    "joblib.dump(clf_only,     MODELS / \"xgb_clf_only.joblib\")\n",
    "\n",
    "# (opcional pero MUY útil) guardar nombres de features transformadas\n",
    "feat_names = list(preprocessor.get_feature_names_out())\n",
    "with open(MODELS / \"feature_names.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(feat_names, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✓ Guardados:\")\n",
    "print(\" -\", MODELS / \"preprocessor.joblib\")\n",
    "print(\" -\", MODELS / \"xgb_clf_only.joblib\")\n",
    "print(\" -\", MODELS / \"feature_names.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e03708c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: c:\\Users\\juana\\MLOPS\\miniproyecto6\\docs\n",
      "MODELS_DIR  : C:\\Users\\juana\\MLOPS\\miniproyecto6\\docs\\models\n",
      "RAW_COLS (del preprocessor): 20\n",
      "TRANSFORMED_NAMES: 45\n"
     ]
    }
   ],
   "source": [
    "# ======== Inference mínimo con preprocessor + xgb_clf_only ========\n",
    "from pathlib import Path\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- util: encontrar la carpeta del proyecto (busca 'models/') ----------\n",
    "def find_project_root(marker=\"models\", max_up=6):\n",
    "    p = Path.cwd()\n",
    "    for _ in range(max_up):\n",
    "        if (p/marker).exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    raise FileNotFoundError(f\"No pude encontrar la carpeta '{marker}' subiendo desde {Path.cwd()}\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root(\"models\")\n",
    "MODELS_DIR   = (PROJECT_ROOT / \"models\").resolve()\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"MODELS_DIR  :\", MODELS_DIR)\n",
    "\n",
    "# ---------- carga de artefactos ----------\n",
    "def load_artifacts():\n",
    "    pre = joblib.load(MODELS_DIR / \"preprocessor.joblib\")      # ColumnTransformer/Pipeline de features\n",
    "    clf = joblib.load(MODELS_DIR / \"xgb_clf_only.joblib\")      # clasificador ya entrenado sobre el espacio transformado\n",
    "\n",
    "    # columnas crudas que el preprocesador espera (del fit)\n",
    "    try:\n",
    "        raw_cols = pre.feature_names_in_.tolist()\n",
    "    except Exception:\n",
    "        # fallback: si por alguna razón no existe, intenta leer de JSON (opcional)\n",
    "        raw_cols = None\n",
    "\n",
    "    # nombres de features transformadas (opcional, para trazas/depuración)\n",
    "    try:\n",
    "        with open(MODELS_DIR / \"feature_names.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            transformed_names = json.load(f)\n",
    "    except Exception:\n",
    "        transformed_names = None\n",
    "\n",
    "    return pre, clf, raw_cols, transformed_names\n",
    "\n",
    "preprocessor, clf, RAW_COLS, TRANSFORMED_NAMES = load_artifacts()\n",
    "print(\"RAW_COLS (del preprocessor):\", len(RAW_COLS) if RAW_COLS is not None else None)\n",
    "print(\"TRANSFORMED_NAMES:\", len(TRANSFORMED_NAMES) if TRANSFORMED_NAMES else None)\n",
    "\n",
    "# ---------- util: normalizar entrada a DataFrame crudo con el orden correcto ----------\n",
    "def to_aligned_dataframe(data):\n",
    "    \"\"\"\n",
    "    data: dict (un registro), list[dict] (varios), o DataFrame crudo.\n",
    "    Devuelve un DataFrame con las columnas en el orden que espera el preprocesador.\n",
    "    \"\"\"\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        df = data.copy()\n",
    "    elif isinstance(data, dict):\n",
    "        df = pd.DataFrame([data])\n",
    "    else:\n",
    "        # lista de dicts o algo parecido\n",
    "        df = pd.DataFrame(list(data))\n",
    "\n",
    "    if RAW_COLS is None:\n",
    "        # si no tenemos RAW_COLS, usamos lo que venga (pero es menos seguro)\n",
    "        return df\n",
    "\n",
    "    # Asegurar columnas: crear faltantes y respetar orden\n",
    "    for c in RAW_COLS:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "    # eliminar columnas desconocidas para evitar errores\n",
    "    df = df[RAW_COLS]\n",
    "    return df\n",
    "\n",
    "# ---------- predicción por lotes ----------\n",
    "def predict_batch(records, threshold=0.5):\n",
    "    \"\"\"\n",
    "    records: dict o list[dict] o DataFrame (crudo)\n",
    "    threshold: umbral de decisión para convertir probas en 0/1\n",
    "    \"\"\"\n",
    "    df_raw = to_aligned_dataframe(records)\n",
    "    X = preprocessor.transform(df_raw)            # -> matriz transformada (numpy/scipy)\n",
    "    proba = clf.predict_proba(X)[:, 1]            # probabilidad de clase 1 (Churn)\n",
    "    pred  = (proba >= threshold).astype(int)\n",
    "\n",
    "    out = df_raw.copy()\n",
    "    out[\"proba_churn\"] = proba\n",
    "    out[\"pred\"] = pred\n",
    "    return out\n",
    "\n",
    "# ---------- ejemplo rápido ----------\n",
    "# Ejemplo con un dict (ajústalo a tus columnas crudas reales)\n",
    "# record = {\n",
    "#     \"gender\": \"Female\",\n",
    "#     \"SeniorCitizen\": 0,\n",
    "#     \"Partner\": \"Yes\",\n",
    "#     \"Dependents\": \"No\",\n",
    "#     \"tenure\": 12,\n",
    "#     \"PhoneService\": \"Yes\",\n",
    "#     \"MultipleLines\": \"No\",\n",
    "#     \"InternetService\": \"Fiber optic\",\n",
    "#     \"OnlineSecurity\": \"No\",\n",
    "#     \"OnlineBackup\": \"Yes\",\n",
    "#     \"DeviceProtection\": \"No\",\n",
    "#     \"TechSupport\": \"No\",\n",
    "#     \"StreamingTV\": \"Yes\",\n",
    "#     \"StreamingMovies\": \"Yes\",\n",
    "#     \"Contract\": \"Month-to-month\",\n",
    "#     \"PaperlessBilling\": \"Yes\",\n",
    "#     \"PaymentMethod\": \"Electronic check\",\n",
    "#     \"MonthlyCharges\": 79.85,\n",
    "#     \"TotalCharges\": 332.3\n",
    "# }\n",
    "# print(predict_batch(record))\n",
    "\n",
    "# Si quieres probar con varios:\n",
    "# print(predict_batch([record, record]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
