{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Preprocesamiento de Datos**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. imports y rutas del proyecto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Sección 3: Preprocesamiento ===\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Rutas del proyecto\n",
    "ROOT = Path(\"..\")\n",
    "DATA = ROOT / \"data\"\n",
    "RAW = DATA / \"raw\"\n",
    "PROC = DATA / \"processed\"\n",
    "MODELS = ROOT / \"models\"\n",
    "for p in [PROC, MODELS]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Carga del dataset y normalización básica**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos el parquet que guardaste en el EDA\n",
    "PARQUET = PROC / \"telco_churn.parquet\"\n",
    "CSV = RAW / \"telco_churn.csv\"\n",
    "df = pd.read_parquet(PARQUET) if PARQUET.exists() else pd.read_csv(CSV)\n",
    "\n",
    "# Normalización mínima de nombres (por si no lo hiciste antes)\n",
    "df.columns = (\n",
    "    df.columns\n",
    "      .str.strip()\n",
    "      .str.replace(\" \", \"_\")\n",
    "      .str.replace(\"(\", \"\", regex=False)\n",
    "      .str.replace(\")\", \"\", regex=False)\n",
    ")\n",
    "\n",
    "# Tipos clave\n",
    "TARGET = \"Churn\"\n",
    "ID_COL = \"customerID\" if \"customerID\" in df.columns else \"customerid\"\n",
    "\n",
    "# Asegurar tipos esperados\n",
    "if df[TARGET].dtype == \"O\":\n",
    "    df[TARGET] = df[TARGET].map({\"Yes\":1, \"No\":0}).astype(\"int8\")\n",
    "\n",
    "# TotalCharges a numérico (en este dataset puede venir sucio)\n",
    "if \"TotalCharges\" in df.columns:\n",
    "    df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Este bloque de código:\n",
    "  - Carga el dataset desde el mejor formato disponible (parquet si existe, CSV si no).\n",
    "  - Limpia y estandariza los nombres de las columnas.\n",
    "  - Define de forma explícita:\n",
    "    - La variable objetivo (`Churn`).\n",
    "    - La columna identificadora de cliente (`customerID` o `customerid`).\n",
    "  - Garantiza que:\n",
    "    - `Churn` esté en formato numérico binario 0/1.\n",
    "    - `TotalCharges` sea realmente una variable numérica.\n",
    "- Tras este paso, el DataFrame `df` queda listo para entrar en la fase de **preprocesamiento y modelado**, con menos riesgo de errores por tipos de datos o nombres de columnas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Detección automática de variables numéricas y categóricas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numéricas: 4 ['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges']\n",
      "Categóricas: 15 ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n"
     ]
    }
   ],
   "source": [
    "# Detectores automáticos\n",
    "cat_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "num_cols = df.select_dtypes(include=[np.number, \"bool\"]).columns.tolist()\n",
    "\n",
    "# Quitar columnas que no deben transformarse como categóricas\n",
    "if ID_COL in cat_cols: cat_cols.remove(ID_COL)\n",
    "if TARGET in cat_cols: cat_cols.remove(TARGET)\n",
    "if TARGET in num_cols: num_cols.remove(TARGET)\n",
    "\n",
    "print(\"Numéricas:\", len(num_cols), num_cols)\n",
    "print(\"Categóricas:\", len(cat_cols), cat_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Detectores automáticos de tipo\n",
    "\n",
    "- Se construyen dos listas a partir de los tipos de datos de `df`:\n",
    "\n",
    "  - `cat_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()`\n",
    "    - Columnas con tipo `object` → se consideran **categóricas**.\n",
    "\n",
    "  - `num_cols = df.select_dtypes(include=[np.number, \"bool\"]).columns.tolist()`\n",
    "    - Columnas numéricas (`int`, `float`) y booleanas → se consideran **numéricas**.\n",
    "\n",
    "\n",
    "###  Quitar columnas que no deben transformarse\n",
    "\n",
    "- Se limpia cada lista para evitar transformar columnas especiales:\n",
    "\n",
    "  - `if ID_COL in cat_cols: cat_cols.remove(ID_COL)`\n",
    "    - El ID del cliente (`customerID`) no se debe codificar; solo identifica filas.\n",
    "\n",
    "  - `if TARGET in cat_cols: cat_cols.remove(TARGET)`\n",
    "  - `if TARGET in num_cols: num_cols.remove(TARGET)`\n",
    "    - El target (`Churn`) no debe aparecer ni en la lista de categóricas ni en la de numéricas\n",
    "      para que **no se le aplique ni OneHotEncoder ni StandardScaler**.\n",
    "\n",
    "- Con esto, el preprocesamiento solo se aplica a **features** y nunca al identificador ni a la variable objetivo.\n",
    "\n",
    "\n",
    "###  Resumen de columnas detectadas\n",
    "\n",
    "Al final se imprimen las listas resultantes:\n",
    "\n",
    "- `print(\"Numéricas:\", len(num_cols), num_cols)`\n",
    "- `print(\"Categóricas:\", len(cat_cols), cat_cols)`\n",
    "\n",
    "La salida que se observa es:\n",
    "\n",
    "- **Numéricas: 4**  \n",
    "  `['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges']`\n",
    "\n",
    "- **Categóricas: 15**  \n",
    "  `['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService',\n",
    "    'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',\n",
    "    'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. División de datos en entrenamiento y prueba (train/test split)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (5634, 20) 0.265\n",
      "Test:  (1409, 20) 0.265\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET].astype(\"int8\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, y_train.mean().round(3))\n",
    "print(\"Test: \", X_test.shape, y_test.mean().round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El conjunto de entrenamiento contiene 5634 filas y 20 variables.\n",
    "- El conjunto de prueba contiene 1409 filas y 20 variables.\n",
    "- La media del target (Churn) es 0.265 en ambos conjuntos.\n",
    "  Esto significa que el 26.5% de los clientes tienen Churn (=1),\n",
    "  y la estratificación funcionó correctamente.\n",
    "- La suma 5634 + 1409 = 7043 confirma que no se perdió ningún registro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Tratamiento de outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class ManejoAtipicos(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, metodo=\"percentil\", estrategia=\"winsorizar\",\n",
    "                 columns=None, lower=0.01, upper=0.99):\n",
    "        self.metodo = metodo\n",
    "        self.estrategia = estrategia   # 'winsorizar' recomendado en pipeline\n",
    "        self.columns = columns         # lista de nombres de columnas numéricas\n",
    "        self.lower = float(lower)\n",
    "        self.upper = float(upper)\n",
    "        self.limites_ = {}\n",
    "        self.feature_names_in_ = None\n",
    "        self.dtypes_ = None\n",
    "        self.transform_cols_ = None\n",
    "        self.last_mask_ = None\n",
    "\n",
    "    # --- helpers ---\n",
    "    def _to_frame(self, X):\n",
    "        \"\"\"Devuelve un DataFrame con nombres de columnas consistentes.\"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            return X.copy()\n",
    "        X = np.asarray(X)\n",
    "        if self.columns is not None:\n",
    "            cols = list(self.columns)\n",
    "        elif self.feature_names_in_ is not None:\n",
    "            cols = list(self.feature_names_in_)\n",
    "        else:\n",
    "            cols = [f\"x{i}\" for i in range(X.shape[1])]\n",
    "        return pd.DataFrame(X, columns=cols).copy()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self._to_frame(X)\n",
    "        # Si no especificaste columns, usa todas las numéricas\n",
    "        cols = list(self.columns) if self.columns is not None else \\\n",
    "               X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "        self.feature_names_in_ = list(X.columns)\n",
    "        self.transform_cols_ = cols\n",
    "        self.dtypes_ = X.dtypes.to_dict()\n",
    "\n",
    "        self.limites_.clear()\n",
    "        for col in cols:\n",
    "            s = pd.to_numeric(X[col], errors=\"coerce\")\n",
    "            if self.metodo == \"iqr\":\n",
    "                q1, q3 = s.quantile(0.25), s.quantile(0.75)\n",
    "                iqr = q3 - q1\n",
    "                li, ls = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "            elif self.metodo == \"percentil\":\n",
    "                li, ls = s.quantile(self.lower), s.quantile(self.upper)\n",
    "            else:\n",
    "                raise ValueError(\"Método no soportado\")\n",
    "            self.limites_[col] = (li, ls)\n",
    "        return self\n",
    "\n",
    "    def row_mask(self, X):\n",
    "        \"\"\"Máscara booleana (True = fila dentro de límites). Útil si deseas ELIMINAR fuera del pipeline.\"\"\"\n",
    "        X = self._to_frame(X)\n",
    "        mask = np.ones(len(X), dtype=bool)\n",
    "        for col in self.transform_cols_:\n",
    "            s = pd.to_numeric(X[col], errors=\"coerce\")\n",
    "            li, ls = self.limites_[col]\n",
    "            mask &= (s >= li) & (s <= ls)\n",
    "        return mask\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = self._to_frame(X)\n",
    "        cols = [c for c in (self.transform_cols_ or []) if c in X.columns]\n",
    "\n",
    "        if self.estrategia == \"winsorizar\":\n",
    "            for col in cols:\n",
    "                s = pd.to_numeric(X[col], errors=\"coerce\")\n",
    "                li, ls = self.limites_[col]\n",
    "                X[col] = np.clip(s, li, ls)\n",
    "            # Restaurar dtypes donde aplique\n",
    "            for c in X.columns:\n",
    "                try:\n",
    "                    X[c] = X[c].astype(self.dtypes_.get(c, X[c].dtype))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            return X.values  # devolver ndarray para jugar bien con ColumnTransformer\n",
    "\n",
    "        elif self.estrategia == \"eliminar\":\n",
    "            # ⚠️ No usar dentro de un Pipeline (cambia n_muestras).\n",
    "            mask = self.row_mask(X)\n",
    "            self.last_mask_ = mask\n",
    "            X = X.loc[mask].reset_index(drop=True)\n",
    "            for c in X.columns:\n",
    "                try:\n",
    "                    X[c] = X[c].astype(self.dtypes_.get(c, X[c].dtype))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            return X.values\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Estrategia no soportada\")\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names_in_ if input_features is None else input_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo principal es:\n",
    "\n",
    "- Calcular límites inferiores y superiores para cada variable numérica (por percentiles o por IQR).\n",
    "- Aplicar una **estrategia** sobre los valores que se salen de esos límites:\n",
    "  - `\"winsorizar\"` → recortar los valores extremos al límite (mantiene todas las filas).\n",
    "  \n",
    "  \n",
    " `ManejoAtipicos` es un transformer robusto para tratar outliers dentro de pipelines de scikit-learn.\n",
    "- Soporta:\n",
    "  - Cálculo de límites por IQR o por percentiles.\n",
    "  - Dos estrategias: winsorizar (recortar) o eliminar filas.\n",
    "- Mantiene:\n",
    "  - Nombres de columnas.\n",
    "  - Tipos de datos originales cuando es posible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Pipeline de Preprocesamiento: Imputación, Escalado y One-Hot Encoding**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_numeric = False  # <- cambia a True si vas a usar modelos que lo requieran\n",
    "\n",
    "numeric_steps = [\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "]\n",
    "if scale_numeric:\n",
    "    numeric_steps.append((\"scaler\", StandardScaler()))\n",
    "\n",
    "numeric_transformer = Pipeline(steps=numeric_steps) if scale_numeric else SimpleImputer(strategy=\"median\")\n",
    "\n",
    "categorical_transformer = OneHotEncoder(\n",
    "    handle_unknown=\"ignore\",\n",
    "    sparse_output=False\n",
    ")\n",
    "\n",
    "# ColumnTransformer que aplica lo anterior por tipo de columna\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\",  numeric_transformer, num_cols),\n",
    "        (\"cat\",  categorical_transformer, cat_cols)\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que produce este preprocesador al usarse con .fit_transform(X):\n",
    "\n",
    "- Para columnas numéricas:\n",
    "     Imputa valores faltantes con la mediana.\n",
    "     (Opcional) Estandariza si scale_numeric=True.\n",
    "\n",
    "- Para columnas categóricas:\n",
    "     Imputa NaN automáticamente (OneHotEncoder lo maneja internamente).\n",
    "     Genera columnas dummy usando One-Hot Encoding.\n",
    "     Ignora categorías desconocidas durante inferencia.\n",
    "\n",
    "- El ColumnTransformer genera un ARRAY NumPy con todas las columnas procesadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Aplicar el preprocesador a Train/Test y reconstruir DataFrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes procesados:\n",
      "X_train: (5634, 45) | X_test: (1409, 45)\n"
     ]
    }
   ],
   "source": [
    "# Ajustar SOLO con train\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Transformar\n",
    "X_train_t = preprocessor.transform(X_train)\n",
    "X_test_t  = preprocessor.transform(X_test)\n",
    "\n",
    "# Recuperar nombres de features tras OHE\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "X_train_df = pd.DataFrame(X_train_t, columns=feature_names, index=X_train.index)\n",
    "X_test_df  = pd.DataFrame(X_test_t,  columns=feature_names, index=X_test.index)\n",
    "\n",
    "print(\"Shapes procesados:\")\n",
    "print(\"X_train:\", X_train_df.shape, \"| X_test:\", X_test_df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestra el código que:\n",
    "1. **Ajusta** el `preprocessor` **solo** con los datos de entrenamiento.\n",
    "2. **Transforma** `X_train` y `X_test` usando el mismo preprocesador.\n",
    "3. **Reconstruye** DataFrames con nombres de columnas después del One-Hot Encoding.\n",
    "4. **Muestra** las dimensiones finales de los conjuntos procesados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8. Guardar datasets procesados, preprocesador**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardados:\n",
      " - C:\\Users\\juana\\MLOPS\\miniproyecto6\\data\\processed\\X_train.parquet\n",
      " - C:\\Users\\juana\\MLOPS\\miniproyecto6\\data\\processed\\X_test.parquet\n",
      " - C:\\Users\\juana\\MLOPS\\miniproyecto6\\data\\processed\\y_train.parquet\n",
      " - C:\\Users\\juana\\MLOPS\\miniproyecto6\\data\\processed\\y_test.parquet\n",
      "Preprocessor guardado en: C:\\Users\\juana\\MLOPS\\miniproyecto6\\models\\preprocessor.joblib\n"
     ]
    }
   ],
   "source": [
    "# Guardar datasets procesados\n",
    "X_train_df.to_parquet(PROC/\"X_train.parquet\", index=False)\n",
    "X_test_df.to_parquet(PROC/\"X_test.parquet\", index=False)\n",
    "y_train.to_frame(name=TARGET).to_parquet(PROC/\"y_train.parquet\", index=False)\n",
    "y_test.to_frame(name=TARGET).to_parquet(PROC/\"y_test.parquet\", index=False)\n",
    "\n",
    "print(\"Guardados:\")\n",
    "print(\" -\", (PROC/\"X_train.parquet\").resolve())\n",
    "print(\" -\", (PROC/\"X_test.parquet\").resolve())\n",
    "print(\" -\", (PROC/\"y_train.parquet\").resolve())\n",
    "print(\" -\", (PROC/\"y_test.parquet\").resolve())\n",
    "\n",
    "# Guardar el preprocesador (útil para GridSearchCV y FastAPI)\n",
    "joblib.dump(preprocessor, MODELS/\"preprocessor.joblib\")\n",
    "print(\"Preprocessor guardado en:\", (MODELS/\"preprocessor.joblib\").resolve())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(0), np.int64(0))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df.isna().sum().sum(), X_test_df.isna().sum().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- Se guardan **cuatro archivos Parquet**:\n",
    "  - `X_train.parquet` → features de entrenamiento procesadas.  \n",
    "  - `X_test.parquet` → features de prueba procesadas.  \n",
    "  - `y_train.parquet` → variable objetivo de entrenamiento.  \n",
    "  - `y_test.parquet` → variable objetivo de prueba.  \n",
    "- También se guarda el objeto `preprocessor` ya ajustado en:\n",
    "  - `models/preprocessor.joblib`, lo que permite:\n",
    "    - Reutilizar exactamente el mismo preprocesamiento en producción (FastAPI, etc.).\n",
    "    - Usarlo dentro de pipelines y GridSearch sin volver a definir toda la lógica.\n",
    "\n",
    "- La última línea devuelve `(np.int64(0), np.int64(0))`, que significa:\n",
    "  - `0` valores faltantes en `X_train_df`.\n",
    "  - `0` valores faltantes en `X_test_df`.\n",
    "- En otras palabras, **todos los NaNs han sido tratados correctamente** en los datos procesados y el dataset está listo para alimentar modelos de Machine Learning sin errores por valores faltantes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
