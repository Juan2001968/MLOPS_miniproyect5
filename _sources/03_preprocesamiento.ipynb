{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Preprocesamiento de Datos**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Sección 3: Preprocesamiento ===\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Rutas del proyecto\n",
    "ROOT = Path(\"..\")\n",
    "DATA = ROOT / \"data\"\n",
    "RAW = DATA / \"raw\"\n",
    "PROC = DATA / \"processed\"\n",
    "MODELS = ROOT / \"models\"\n",
    "for p in [PROC, MODELS]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos el parquet que guardaste en el EDA\n",
    "PARQUET = PROC / \"telco_churn.parquet\"\n",
    "CSV = RAW / \"telco_churn.csv\"\n",
    "df = pd.read_parquet(PARQUET) if PARQUET.exists() else pd.read_csv(CSV)\n",
    "\n",
    "# Normalización mínima de nombres (por si no lo hiciste antes)\n",
    "df.columns = (\n",
    "    df.columns\n",
    "      .str.strip()\n",
    "      .str.replace(\" \", \"_\")\n",
    "      .str.replace(\"(\", \"\", regex=False)\n",
    "      .str.replace(\")\", \"\", regex=False)\n",
    ")\n",
    "\n",
    "# Tipos clave\n",
    "TARGET = \"Churn\"\n",
    "ID_COL = \"customerID\" if \"customerID\" in df.columns else \"customerid\"\n",
    "\n",
    "# Asegurar tipos esperados\n",
    "if df[TARGET].dtype == \"O\":\n",
    "    df[TARGET] = df[TARGET].map({\"Yes\":1, \"No\":0}).astype(\"int8\")\n",
    "\n",
    "# TotalCharges a numérico (en este dataset puede venir sucio)\n",
    "if \"TotalCharges\" in df.columns:\n",
    "    df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numéricas: 4 ['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges']\n",
      "Categóricas: 15 ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n"
     ]
    }
   ],
   "source": [
    "# Detectores automáticos\n",
    "cat_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "num_cols = df.select_dtypes(include=[np.number, \"bool\"]).columns.tolist()\n",
    "\n",
    "# Quitar columnas que no deben transformarse como categóricas\n",
    "if ID_COL in cat_cols: cat_cols.remove(ID_COL)\n",
    "if TARGET in cat_cols: cat_cols.remove(TARGET)\n",
    "if TARGET in num_cols: num_cols.remove(TARGET)\n",
    "\n",
    "print(\"Numéricas:\", len(num_cols), num_cols)\n",
    "print(\"Categóricas:\", len(cat_cols), cat_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (5634, 20) 0.265\n",
      "Test:  (1409, 20) 0.265\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET].astype(\"int8\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, y_train.mean().round(3))\n",
    "print(\"Test: \", X_test.shape, y_test.mean().round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class ManejoAtipicos(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, metodo=\"percentil\", estrategia=\"winsorizar\",\n",
    "                 columns=None, lower=0.01, upper=0.99):\n",
    "        self.metodo = metodo\n",
    "        self.estrategia = estrategia   # 'winsorizar' recomendado en pipeline\n",
    "        self.columns = columns         # lista de nombres de columnas numéricas\n",
    "        self.lower = float(lower)\n",
    "        self.upper = float(upper)\n",
    "        self.limites_ = {}\n",
    "        self.feature_names_in_ = None\n",
    "        self.dtypes_ = None\n",
    "        self.transform_cols_ = None\n",
    "        self.last_mask_ = None\n",
    "\n",
    "    # --- helpers ---\n",
    "    def _to_frame(self, X):\n",
    "        \"\"\"Devuelve un DataFrame con nombres de columnas consistentes.\"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            return X.copy()\n",
    "        X = np.asarray(X)\n",
    "        if self.columns is not None:\n",
    "            cols = list(self.columns)\n",
    "        elif self.feature_names_in_ is not None:\n",
    "            cols = list(self.feature_names_in_)\n",
    "        else:\n",
    "            cols = [f\"x{i}\" for i in range(X.shape[1])]\n",
    "        return pd.DataFrame(X, columns=cols).copy()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self._to_frame(X)\n",
    "        # Si no especificaste columns, usa todas las numéricas\n",
    "        cols = list(self.columns) if self.columns is not None else \\\n",
    "               X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "        self.feature_names_in_ = list(X.columns)\n",
    "        self.transform_cols_ = cols\n",
    "        self.dtypes_ = X.dtypes.to_dict()\n",
    "\n",
    "        self.limites_.clear()\n",
    "        for col in cols:\n",
    "            s = pd.to_numeric(X[col], errors=\"coerce\")\n",
    "            if self.metodo == \"iqr\":\n",
    "                q1, q3 = s.quantile(0.25), s.quantile(0.75)\n",
    "                iqr = q3 - q1\n",
    "                li, ls = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "            elif self.metodo == \"percentil\":\n",
    "                li, ls = s.quantile(self.lower), s.quantile(self.upper)\n",
    "            else:\n",
    "                raise ValueError(\"Método no soportado\")\n",
    "            self.limites_[col] = (li, ls)\n",
    "        return self\n",
    "\n",
    "    def row_mask(self, X):\n",
    "        \"\"\"Máscara booleana (True = fila dentro de límites). Útil si deseas ELIMINAR fuera del pipeline.\"\"\"\n",
    "        X = self._to_frame(X)\n",
    "        mask = np.ones(len(X), dtype=bool)\n",
    "        for col in self.transform_cols_:\n",
    "            s = pd.to_numeric(X[col], errors=\"coerce\")\n",
    "            li, ls = self.limites_[col]\n",
    "            mask &= (s >= li) & (s <= ls)\n",
    "        return mask\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = self._to_frame(X)\n",
    "        cols = [c for c in (self.transform_cols_ or []) if c in X.columns]\n",
    "\n",
    "        if self.estrategia == \"winsorizar\":\n",
    "            for col in cols:\n",
    "                s = pd.to_numeric(X[col], errors=\"coerce\")\n",
    "                li, ls = self.limites_[col]\n",
    "                X[col] = np.clip(s, li, ls)\n",
    "            # Restaurar dtypes donde aplique\n",
    "            for c in X.columns:\n",
    "                try:\n",
    "                    X[c] = X[c].astype(self.dtypes_.get(c, X[c].dtype))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            return X.values  # devolver ndarray para jugar bien con ColumnTransformer\n",
    "\n",
    "        elif self.estrategia == \"eliminar\":\n",
    "            # ⚠️ No usar dentro de un Pipeline (cambia n_muestras).\n",
    "            mask = self.row_mask(X)\n",
    "            self.last_mask_ = mask\n",
    "            X = X.loc[mask].reset_index(drop=True)\n",
    "            for c in X.columns:\n",
    "                try:\n",
    "                    X[c] = X[c].astype(self.dtypes_.get(c, X[c].dtype))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            return X.values\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Estrategia no soportada\")\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names_in_ if input_features is None else input_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_numeric = False  # <- cambia a True si vas a usar modelos que lo requieran\n",
    "\n",
    "numeric_steps = [\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "]\n",
    "if scale_numeric:\n",
    "    numeric_steps.append((\"scaler\", StandardScaler()))\n",
    "\n",
    "numeric_transformer = Pipeline(steps=numeric_steps) if scale_numeric else SimpleImputer(strategy=\"median\")\n",
    "\n",
    "categorical_transformer = OneHotEncoder(\n",
    "    handle_unknown=\"ignore\",\n",
    "    sparse_output=False\n",
    ")\n",
    "\n",
    "# ColumnTransformer que aplica lo anterior por tipo de columna\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\",  numeric_transformer, num_cols),\n",
    "        (\"cat\",  categorical_transformer, cat_cols)\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes procesados:\n",
      "X_train: (5634, 45) | X_test: (1409, 45)\n"
     ]
    }
   ],
   "source": [
    "# Ajustar SOLO con train\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Transformar\n",
    "X_train_t = preprocessor.transform(X_train)\n",
    "X_test_t  = preprocessor.transform(X_test)\n",
    "\n",
    "# Recuperar nombres de features tras OHE\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "X_train_df = pd.DataFrame(X_train_t, columns=feature_names, index=X_train.index)\n",
    "X_test_df  = pd.DataFrame(X_test_t,  columns=feature_names, index=X_test.index)\n",
    "\n",
    "print(\"Shapes procesados:\")\n",
    "print(\"X_train:\", X_train_df.shape, \"| X_test:\", X_test_df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardados:\n",
      " - C:\\Users\\juana\\MLOPS\\miniproyecto6\\data\\processed\\X_train.parquet\n",
      " - C:\\Users\\juana\\MLOPS\\miniproyecto6\\data\\processed\\X_test.parquet\n",
      " - C:\\Users\\juana\\MLOPS\\miniproyecto6\\data\\processed\\y_train.parquet\n",
      " - C:\\Users\\juana\\MLOPS\\miniproyecto6\\data\\processed\\y_test.parquet\n",
      "Preprocessor guardado en: C:\\Users\\juana\\MLOPS\\miniproyecto6\\models\\preprocessor.joblib\n"
     ]
    }
   ],
   "source": [
    "# Guardar datasets procesados\n",
    "X_train_df.to_parquet(PROC/\"X_train.parquet\", index=False)\n",
    "X_test_df.to_parquet(PROC/\"X_test.parquet\", index=False)\n",
    "y_train.to_frame(name=TARGET).to_parquet(PROC/\"y_train.parquet\", index=False)\n",
    "y_test.to_frame(name=TARGET).to_parquet(PROC/\"y_test.parquet\", index=False)\n",
    "\n",
    "print(\"Guardados:\")\n",
    "print(\" -\", (PROC/\"X_train.parquet\").resolve())\n",
    "print(\" -\", (PROC/\"X_test.parquet\").resolve())\n",
    "print(\" -\", (PROC/\"y_train.parquet\").resolve())\n",
    "print(\" -\", (PROC/\"y_test.parquet\").resolve())\n",
    "\n",
    "# Guardar el preprocesador (útil para GridSearchCV y FastAPI)\n",
    "joblib.dump(preprocessor, MODELS/\"preprocessor.joblib\")\n",
    "print(\"Preprocessor guardado en:\", (MODELS/\"preprocessor.joblib\").resolve())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(0), np.int64(0))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df.isna().sum().sum(), X_test_df.isna().sum().sum()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
